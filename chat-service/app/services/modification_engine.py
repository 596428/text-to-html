"""
Modification Engine

Responsibilities:
- Generate HTML modifications based on user request
- Create Patch objects for local changes
- Generate full HTML for global changes
- Handle special cases like translation

Dependencies:
- app.services.gemini_client
- app.models.chat (Patch, PatchAction, ChatResponse, ChatResponseType)
- app.models.common (AnalysisResult, IntentType, ChangeType)

Implementation Notes:
- Use different strategies for local vs global changes
- Parse Patch JSON from LLM response
- Optimize translation by extracting text nodes
- Always include summary message
"""

from typing import List, Dict, Optional, Any
import json
import time
import logging

from app.models.chat import Patch, PatchAction, ChatResponse, ChatResponseType
from app.models.common import AnalysisResult, IntentType, ChangeType
from app.services.gemini_client import GeminiClient

logger = logging.getLogger(__name__)


class ModificationEngine:
    """
    Generate HTML modifications

    Usage:
        engine = ModificationEngine()

        # For local changes
        response = await engine.process_local_change(
            message="Ìó§Îçî ÌååÎûÄÏÉâÏúºÎ°ú",
            context={"header": "<header>...</header>"},
            analysis=analysis_result
        )

        # For global changes
        response = await engine.process_global_change(
            message="Î™®Îì† ÌÖçÏä§Ìä∏ ÏòÅÏñ¥Î°ú",
            full_html="<!DOCTYPE html>...",
            analysis=analysis_result
        )
    """

    def __init__(self):
        """Initialize modification engine"""
        self.gemini = GeminiClient()

    async def process_local_change(
        self,
        message: str,
        context: Dict[str, str],
        analysis: AnalysisResult
    ) -> ChatResponse:
        """
        Process local change request

        Args:
            message: User message
            context: Section ID -> HTML mapping
            analysis: Intent analysis result

        Returns:
            ChatResponse with patches
        """
        start_time = time.time()

        try:
            # Build prompt for patch generation
            prompt = self._build_local_change_prompt(message, context, analysis)

            # Call Gemini
            result = await self.gemini.generate_content(
                prompt=prompt,
                temperature=0.3  # Lower temperature for more consistent JSON
            )

            # Parse patches and summary from response
            patches, summary = self._parse_patches(result["text"])

            processing_time = time.time() - start_time

            # Use summary from LLM, fallback to default message
            response_message = summary if summary else f"ÏàòÏ†ï Ìå®Ïπò {len(patches)}Í∞ú ÏÉùÏÑ±Îê®"

            return ChatResponse(
                type=ChatResponseType.PATCH,
                patches=patches,
                message=response_message,
                metadata={
                    "tokens_used": result.get("tokens_used", 0),
                    "processing_time": processing_time
                }
            )

        except Exception as e:
            logger.error(f"Local change processing failed: {e}", exc_info=True)
            processing_time = time.time() - start_time
            return self._create_error_response(
                f"ÏàòÏ†ï Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}",
                processing_time
            )

    async def process_global_change(
        self,
        message: str,
        full_html: str,
        analysis: AnalysisResult
    ) -> ChatResponse:
        """
        Process global change request

        Args:
            message: User message
            full_html: Full HTML document
            analysis: Intent analysis result

        Returns:
            ChatResponse with modified HTML
        """
        start_time = time.time()

        try:
            # Special handling for translation
            if analysis.change_type == ChangeType.TRANSLATION:
                return await self._process_translation(message, full_html)

            # Build prompt for global modification
            prompt = self._build_global_change_prompt(message, full_html, analysis)

            # Call Gemini
            result = await self.gemini.generate_content(
                prompt=prompt,
                temperature=0.5
            )

            # Extract clean HTML from response
            modified_html = self._extract_html(result["text"])

            processing_time = time.time() - start_time

            return ChatResponse(
                type=ChatResponseType.FULL,
                html=modified_html,
                message="Ï†ÑÏ≤¥ HTML ÏàòÏ†ï ÏôÑÎ£å",
                metadata={
                    "tokens_used": result.get("tokens_used", 0),
                    "processing_time": processing_time
                }
            )

        except Exception as e:
            logger.error(f"Global change processing failed: {e}", exc_info=True)
            processing_time = time.time() - start_time
            return self._create_error_response(
                f"Ï†ÑÏ≤¥ ÏàòÏ†ï Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}",
                processing_time
            )

    async def process_query(
        self,
        message: str,
        context_html: Optional[str] = None
    ) -> ChatResponse:
        """
        Process query request (HTML-related question without modification)

        Args:
            message: User question
            context_html: Optional HTML context for the question

        Returns:
            ChatResponse with answer message (MESSAGE type)
        """
        start_time = time.time()

        try:
            # Build query prompt
            if context_html:
                prompt = f"""Îã§Ïùå HTMLÏóê ÎåÄÌïú ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî.

## HTML Ïª®ÌÖçÏä§Ìä∏
{context_html}

## ÏßàÎ¨∏
"{message}"

## ÏùëÎãµ
Í∞ÑÍ≤∞ÌïòÍ≥† Î™ÖÌôïÌïòÍ≤å ÎãµÎ≥ÄÌïòÏÑ∏Ïöî. HTMLÏùÑ ÏàòÏ†ïÌïòÏßÄ ÎßêÍ≥†, ÏßàÎ¨∏Ïóê ÎåÄÌïú Ï†ïÎ≥¥Îßå Ï†úÍ≥µÌïòÏÑ∏Ïöî."""
            else:
                prompt = f"""Îã§Ïùå ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî.

## ÏßàÎ¨∏
"{message}"

## ÏùëÎãµ
Í∞ÑÍ≤∞ÌïòÍ≥† Î™ÖÌôïÌïòÍ≤å ÎãµÎ≥ÄÌïòÏÑ∏Ïöî."""

            # Call Gemini
            result = await self.gemini.generate_content(
                prompt=prompt,
                temperature=0.7
            )

            processing_time = time.time() - start_time

            # Return as MESSAGE type (no HTML modification)
            return ChatResponse(
                type=ChatResponseType.MESSAGE,
                message=result["text"].strip(),
                metadata={
                    "tokens_used": result.get("tokens_used", 0),
                    "processing_time": processing_time
                }
            )

        except Exception as e:
            logger.error(f"Query processing failed: {e}", exc_info=True)
            processing_time = time.time() - start_time
            return self._create_error_response(
                f"ÏßàÎ¨∏ Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}",
                processing_time
            )

    def process_off_topic(self) -> ChatResponse:
        """
        Process off-topic request (not related to HTML modification)

        Returns a polite decline message without calling Gemini API.

        Returns:
            ChatResponse with decline message (MESSAGE type)
        """
        return ChatResponse(
            type=ChatResponseType.MESSAGE,
            message="Ï£ÑÏÜ°Ìï©ÎãàÎã§. Ï†ÄÎäî HTML ÏàòÏ†ï Ï†ÑÏö© Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. \"Ìó§Îçî Î∞∞Í≤ΩÏÉâÏùÑ ÌååÎûÄÏÉâÏúºÎ°ú Î∞îÍøîÏ§ò\"ÏôÄ Í∞ôÏù¥ HTML ÏàòÏ†ïÍ≥º Í¥ÄÎ†®Îêú ÏöîÏ≤≠ÏùÑ Ìï¥Ï£ºÏÑ∏Ïöî.",
            metadata={
                "tokens_used": 0,
                "processing_time": 0.0
            }
        )

    def process_unclear(self) -> ChatResponse:
        """
        Process unclear request (HTML-related but needs clarification)

        Returns a message asking for more details.

        Returns:
            ChatResponse with clarification request (MESSAGE type)
        """
        return ChatResponse(
            type=ChatResponseType.MESSAGE,
            message="ÏöîÏ≤≠Ïù¥ Î∂àÎ™ÖÌôïÌï©ÎãàÎã§. Ïñ¥Îñ§ Î∂ÄÎ∂ÑÏùÑ Ïñ¥ÎñªÍ≤å ÏàòÏ†ïÌïòÎ©¥ Ï¢ãÏùÑÏßÄ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÏïåÎ†§Ï£ºÏÑ∏Ïöî. Ïòà: \"Ìó§ÎçîÏùò Î∞∞Í≤ΩÏÉâÏùÑ ÌååÎûÄÏÉâÏúºÎ°ú Î≥ÄÍ≤ΩÌï¥Ï§ò\", \"Î©îÏù∏ ÌÉÄÏù¥ÌãÄ ÌÖçÏä§Ìä∏Î•º 'ÌôòÏòÅÌï©ÎãàÎã§'Î°ú Î∞îÍøîÏ§ò\"",
            metadata={
                "tokens_used": 0,
                "processing_time": 0.0
            }
        )

    async def _process_translation(
        self,
        message: str,
        full_html: str
    ) -> ChatResponse:
        """
        Process translation request efficiently

        Extracts text nodes and translates them separately
        for better token efficiency.

        Args:
            message: User message
            full_html: Full HTML

        Returns:
            ChatResponse with translated HTML
        """
        start_time = time.time()

        try:
            from bs4 import BeautifulSoup

            # Parse HTML
            soup = BeautifulSoup(full_html, 'html.parser')

            # Extract all text nodes (non-empty)
            text_nodes = []
            for text in soup.find_all(string=True):
                if text.strip() and text.parent.name not in ['script', 'style']:
                    text_nodes.append(text.strip())

            # Remove duplicates while preserving order
            unique_texts = list(dict.fromkeys(text_nodes))

            if not unique_texts:
                return self._create_error_response("Î≤àÏó≠Ìï† ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§")

            # Build translation prompt
            prompt = f"""Îã§Ïùå ÌÖçÏä§Ìä∏Îì§ÏùÑ Î≤àÏó≠ÌïòÏÑ∏Ïöî.

ÏöîÏ≤≠: "{message}"

ÌÖçÏä§Ìä∏ Î™©Î°ù:
{chr(10).join(f"{i+1}. {text}" for i, text in enumerate(unique_texts))}

ÏùëÎãµ ÌòïÏãù (JSON):
{{
    "translations": [
        "Î≤àÏó≠Îêú ÌÖçÏä§Ìä∏ 1",
        "Î≤àÏó≠Îêú ÌÖçÏä§Ìä∏ 2",
        ...
    ]
}}

Ï£ºÏùò: ÏàúÏÑúÎ•º Ï†ïÌôïÌûà Ïú†ÏßÄÌïòÍ≥†, HTML ÌÉúÍ∑∏Îäî Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî."""

            # Call Gemini for translation
            result = await self.gemini.generate_content(
                prompt=prompt,
                temperature=0.3
            )

            # Parse translation results
            response_text = result["text"].strip()

            # Remove markdown code blocks if present
            if response_text.startswith("```"):
                lines = response_text.split("\n")
                response_text = "\n".join(lines[1:-1] if len(lines) > 2 else lines)

            # Parse JSON
            try:
                translation_data = json.loads(response_text)
                translations = translation_data.get("translations", [])
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse translation JSON: {e}")
                translations = []

            # Create text replacement mapping
            text_map = {unique_texts[i]: translations[i]
                       for i in range(min(len(unique_texts), len(translations)))}

            # Replace text nodes in HTML
            modified_html = full_html
            for original, translated in text_map.items():
                modified_html = modified_html.replace(original, translated)

            processing_time = time.time() - start_time

            return ChatResponse(
                type=ChatResponseType.FULL,
                html=modified_html,
                message=f"{len(translations)}Í∞ú ÌÖçÏä§Ìä∏ Î≤àÏó≠ ÏôÑÎ£å",
                metadata={
                    "tokens_used": result.get("tokens_used", 0),
                    "processing_time": processing_time
                }
            )

        except Exception as e:
            logger.error(f"Translation processing failed: {e}", exc_info=True)
            processing_time = time.time() - start_time
            return self._create_error_response(
                f"Î≤àÏó≠ Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}",
                processing_time
            )

    def _build_local_change_prompt(
        self,
        message: str,
        context: Dict[str, str],
        analysis: AnalysisResult
    ) -> str:
        """
        Build prompt for local change

        Args:
            message: User message
            context: Section HTML mapping
            analysis: Analysis result

        Returns:
            Prompt string
        """
        # Build HTML sections string
        html_sections = "\n\n".join(
            f"## Section: {section_id}\n{html}"
            for section_id, html in context.items()
        )

        prompt = f"""HTML ÏÑπÏÖòÏùÑ ÏàòÏ†ïÌïòÏÑ∏Ïöî.

## üéØ Í∞ÄÏû• Ï§ëÏöîÌïú Í∑úÏπô
**ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÄÏ†ïÌïú Í∞íÏùÑ Ï†àÎåÄ Î≥ÄÍ≤ΩÌïòÏßÄ ÎßàÏÑ∏Ïöî!**
- ÏÇ¨Ïö©ÏûêÍ∞Ä "Îπ®Í∞ÑÏÉâ"Ïù¥ÎùºÍ≥† ÌïòÎ©¥ ‚Üí Î∞òÎìúÏãú "red" ÏÇ¨Ïö© (orange, crimson Í∏àÏßÄ)
- ÏÇ¨Ïö©ÏûêÍ∞Ä "ÌååÎûÄÏÉâ"Ïù¥ÎùºÍ≥† ÌïòÎ©¥ ‚Üí Î∞òÎìúÏãú "blue" ÏÇ¨Ïö©
- ÏÇ¨Ïö©ÏûêÍ∞Ä "20px"ÎùºÍ≥† ÌïòÎ©¥ ‚Üí Î∞òÎìúÏãú "20px" ÏÇ¨Ïö©
- ÏÇ¨Ïö©ÏûêÍ∞Ä ÌäπÏ†ï ÌÖçÏä§Ìä∏Î•º ÏßÄÏ†ïÌïòÎ©¥ ‚Üí Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©

## ÏàòÏ†ï ÏöîÏ≤≠
"{message}"

## ÎåÄÏÉÅ HTML
{html_sections}

## Î∂ÑÏÑùÎêú ÏûëÏóÖ
- ÎåÄÏÉÅ: {analysis.target_description}
- ÏûëÏóÖ: {analysis.action_description}

## ÏùëÎãµ ÌòïÏãù (JSON)
Îã§Ïùå ÌòïÏãùÏùò JSONÏúºÎ°ú ÏùëÎãµÌïòÏÑ∏Ïöî:

{{
    "patches": [
        {{
            "selector": "CSS ÏÑ†ÌÉùÏûê (Ïòà: #header, .button, div.card)",
            "action": "addClass|removeClass|replaceClass|setText|setHtml|setAttribute|removeAttribute|setStyle|removeElement|appendChild|prependChild",
            "oldValue": "Í∏∞Ï°¥ Í∞í (setText, replaceClass Îì±ÏóêÏÑú ÏÇ¨Ïö©)",
            "newValue": "ÏÉà Í∞í",
            "value": "Ï†ÅÏö©Ìï† Í∞í (Îã®Ïùº Í∞íÏù∏ Í≤ΩÏö∞)"
        }}
    ],
    "summary": "ÏàòÏ†ï ÎÇ¥Ïö© ÏöîÏïΩ"
}}

Ï£ºÏùòÏÇ¨Ìï≠:
1. selectorÎäî Î∞òÎìúÏãú Ïú†Ìö®Ìïú CSS ÏÑ†ÌÉùÏûêÏó¨Ïïº Ìï®
2. setStyle: valueÏóê "ÏÜçÏÑ±Î™Ö: Í∞í" ÌòïÏãù (Ïòà: "background-color: red")
3. setText/setHtml: newValueÏóê ÏÉà ÎÇ¥Ïö©
4. Ïó¨Îü¨ ÏöîÏÜåÎ•º ÏàòÏ†ïÌï¥Ïïº ÌïòÎ©¥ patches Î∞∞Ïó¥Ïóê Ïó¨Îü¨ Ìï≠Î™© Ìè¨Ìï®"""

        return prompt

    def _build_global_change_prompt(
        self,
        message: str,
        full_html: str,
        analysis: AnalysisResult
    ) -> str:
        """
        Build prompt for global change

        Args:
            message: User message
            full_html: Full HTML
            analysis: Analysis result

        Returns:
            Prompt string
        """
        prompt = f"""Ï†ÑÏ≤¥ HTML Î¨∏ÏÑúÎ•º ÏàòÏ†ïÌïòÏÑ∏Ïöî.

## ÌòÑÏû¨ HTML
{full_html}

## ÏàòÏ†ï ÏöîÏ≤≠
"{message}"

## Î∂ÑÏÑùÎêú ÏûëÏóÖ
- ÎåÄÏÉÅ: {analysis.target_description}
- ÏûëÏóÖ: {analysis.action_description}
- Î≥ÄÍ≤Ω Ïú†Ìòï: {analysis.change_type}

## ÏùëÎãµ ÌòïÏãù
ÏàòÏ†ïÎêú ÏôÑÏ†ÑÌïú HTML Î¨∏ÏÑúÎ•º Î∞òÌôòÌïòÏÑ∏Ïöî.
ÎßàÌÅ¨Îã§Ïö¥ ÏΩîÎìú Î∏îÎ°ùÏúºÎ°ú Í∞êÏã∏ÎèÑ Ï¢ãÏäµÎãàÎã§.

Ï£ºÏùòÏÇ¨Ìï≠:
1. HTML Íµ¨Ï°∞Î•º Ïú†ÏßÄÌïòÏÑ∏Ïöî
2. DOCTYPE, html, head, body ÌÉúÍ∑∏Îäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ
3. Í∏∞Ï°¥ IDÏôÄ ÌÅ¥ÎûòÏä§Îäî Í∞ÄÎä•Ìïú Ïú†ÏßÄ
4. ÏöîÏ≤≠Îêú Î≥ÄÍ≤ΩÏÇ¨Ìï≠Îßå Ï†ÅÏö©
5. ÏôÑÏ†ÑÌûà ÏûëÎèôÌïòÎäî HTMLÏùÑ Î∞òÌôò"""

        return prompt

    def _parse_patches(self, response_text: str) -> tuple[List[Patch], str]:
        """
        Parse patches and summary from LLM response

        Args:
            response_text: Raw response

        Returns:
            Tuple of (List of Patch objects, summary string)
        """
        # Remove markdown code blocks if present
        text = response_text.strip()
        if text.startswith("```"):
            lines = text.split("\n")
            # Remove first line (```json or ```) and last line (```)
            text = "\n".join(lines[1:-1] if len(lines) > 2 else lines)
            text = text.strip()

        # Parse JSON
        try:
            data = json.loads(text)
            patches_data = data.get("patches", [])
            summary = data.get("summary", "")

            # Convert to Patch objects
            patches = []
            for patch_dict in patches_data:
                # Validate required fields
                if "selector" not in patch_dict or "action" not in patch_dict:
                    logger.warning(f"Invalid patch (missing selector or action): {patch_dict}")
                    continue

                try:
                    patch = Patch(
                        selector=patch_dict["selector"],
                        action=PatchAction(patch_dict["action"]),
                        old_value=patch_dict.get("oldValue"),
                        new_value=patch_dict.get("newValue"),
                        value=patch_dict.get("value")
                    )
                    patches.append(patch)
                except (ValueError, KeyError) as e:
                    logger.warning(f"Failed to create patch from {patch_dict}: {e}")
                    continue

            return patches, summary

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse patches JSON: {e}")
            logger.error(f"Response text: {text[:500]}")
            return [], ""

    def _extract_html(self, response_text: str) -> str:
        """
        Extract HTML from LLM response

        Remove markdown code blocks if present.

        Args:
            response_text: Raw response

        Returns:
            Clean HTML string
        """
        text = response_text.strip()

        # Remove markdown code blocks
        # Pattern 1: ```html ... ```
        # Pattern 2: ``` ... ```
        if text.startswith("```"):
            lines = text.split("\n")
            # Remove first line and last line
            if len(lines) > 2:
                text = "\n".join(lines[1:-1])
            else:
                text = "\n".join(lines)
            text = text.strip()

        return text

    def _create_error_response(
        self,
        error_message: str,
        processing_time: float = 0.0
    ) -> ChatResponse:
        """
        Create error response

        Args:
            error_message: Error description
            processing_time: Time spent

        Returns:
            ChatResponse with error
        """
        return ChatResponse(
            type=ChatResponseType.ERROR,
            message=error_message,
            metadata={
                "tokens_used": 0,
                "processing_time": processing_time
            }
        )
